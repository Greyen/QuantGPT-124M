TO DO 
- Why need to do the projections of q,k,v in attention head , and these projecions to the headsize.
- have to look what is Xavier normal init and how it is different form normal init
-  Do we also add weights in the normalization
-  bias brodcasting
-  when doing the backpropagation , how to tackle the derative of matrix 
 - make matrix multiplication generic if possible 